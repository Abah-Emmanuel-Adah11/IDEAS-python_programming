{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7923fe9d-5224-4cf6-a6af-054a9bed831e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#The request like an actual individual actually makes a real \"request\" to a site for information.\n",
    "import requests\n",
    "\n",
    "# Web scrapping from an external site, using its url as provided in within the argument of the get function\n",
    "html_text = requests.get('https://m.timesjobs.com/mobile/jobs-search-result.html?txtKeywords=Python%2C&cboWorkExp1=-1&txtLocation=').text\n",
    "soup = BeautifulSoup(html_text, 'lxml')\n",
    "job = soup.find('li', class_ = 'ui-content search-result')\n",
    "print(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d442aa2e-d191-4db5-97d9-9abf1c9896a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.” , Albert Einstein\n",
      "“It is our choices, Harry, that show what we truly are, far more than our abilities.” , J.K. Rowling\n",
      "“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.” , Albert Einstein\n",
      "“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.” , Jane Austen\n",
      "“Imperfection is beauty, madness is genius and it's better to be absolutely ridiculous than absolutely boring.” , Marilyn Monroe\n",
      "“Try not to become a man of success. Rather become a man of value.” , Albert Einstein\n",
      "“It is better to be hated for what you are than to be loved for what you are not.” , André Gide\n",
      "“I have not failed. I've just found 10,000 ways that won't work.” , Thomas A. Edison\n",
      "“A woman is like a tea bag; you never know how strong it is until it's in hot water.” , Eleanor Roosevelt\n",
      "“A day without sunshine is like, you know, night.” , Steve Martin\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "\n",
    "# Information related to the site you want to scrape.\n",
    "page_to_scrape = requests.get(\"https://quotes.toscrape.com\")\n",
    "soup = BeautifulSoup(page_to_scrape.text, \"html.parser\")\n",
    "                     \n",
    "# The first argument of the findAll method is the title of the tag while the second argument is the attribute name and its value e.g class: text.\n",
    "quotes = soup.findAll(\"span\", attrs={\"class\": \"text\"})\n",
    "authors = soup.findAll(\"small\", attrs={\"class\": \"author\"})\n",
    "\n",
    "# Writing the scrapped data into a CSV file to be called \"scrapped_quote.csv\"\n",
    "file = open(\"scraped_quote.csv\", \"w\")\n",
    "writer = csv.writer(file)\n",
    "\n",
    "# Using a for loop to iterate through all the quote.\n",
    "for quote, author in zip(quotes, authors):\n",
    "    print(quote.text + \" , \" + author.text)\n",
    "    # writting the iterated content into the csv file\n",
    "    writer.writerow([quote.text, author.text])\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f7fed1a8-9d49-4ed4-af23-6d1a412e65c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PSG beat Arsenal 3-1 on aggregate to reach Champions League final\n",
      " , Inter Milan come back down to earth on Sunday when they travel to Torino, hoping to relaunch their Serie A title defence, still buzzing from one of the greatest nights of European football in their history.\n",
      "\n",
      "Tottenham star James Maddison to miss rest of season\n",
      " , \n",
      "Paris Saint-Germain forward Ousmane Dembele, who suffered a hamstring injury last week, has been passed fit to play in their Champions League semi-final second leg against Arsenal, coach Luis Enrique said on Tuesday.\n",
      "\n",
      "\n",
      "Inter turn attention to fading Serie A title defence after Barca triumph\n",
      " , \n",
      "Ousmane Dembele and his fellow attacking players may have stolen most of the limelight on Paris Saint-Germain’s run in this season’s Champions League, but the form of goalkeeper Gianluigi Donnarumma has been just as important in taking the French giants to the brink of the final.\n",
      "\n",
      "\n",
      "Dembele starts on bench for PSG against Arsenal in UCL\n",
      " , \n",
      "In an expected turn of events, Inter Milan and Barcelona will settle the scores later tonight in a highly-anticipated semi-final second leg of the UEFA Champions League.\n",
      "\n",
      "\n",
      "Conference League glory would prove Chelsea are back – Maresca\n",
      " , \n",
      "A man in Greece on Tuesday said he needed stitches after allegedly being bitten by the family dog of tennis star Stefanos Tsitsipas.\n",
      "\n",
      "\n",
      "Amorim determined to keep Fernandes at Man Utd despite Saudi interest\n",
      " , \n",
      "Arsenal, who have never won the Champions League, are on the brink of becoming PSG’s latest English scalp this season after a disappointing first leg that could have ended in a heavier defeat.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "\n",
    "# Information related to the site you want to scrape.\n",
    "page_to_scrape = requests.get(\"https://www.vanguardngr.com/category/sports/\")\n",
    "soup = BeautifulSoup(page_to_scrape.text, \"html.parser\")\n",
    "                     \n",
    "# The first argument of the findAll method is the title of the tag while the second argument is the attribute name and its value e.g class: text.\n",
    "head_lines = soup.findAll(\"h3\", attrs={\"class\": \"entry-title\"})\n",
    "#contents = soup.findAll(\"p\")\n",
    "contents = soup.findAll(\"div\", attrs={\"class\": \"entry-excerpt\"})\n",
    "\n",
    "# Writing the scrapped data into a CSV file to be called \"scrapped_quote.csv\"\n",
    "file = open(\"scraped_news.csv\", \"w\")\n",
    "writer = csv.writer(file)\n",
    "\n",
    "# Using a for loop to iterate through all the quote.\n",
    "for head_line, content in zip(head_lines, contents):\n",
    "    print(head_line.text + \" , \" + content.text)\n",
    "    #writting the iterated content into the csv file\n",
    "    writer.writerow([head_line.text, content.text])\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "af0a252a-1174-4800-bce3-4ad07e49e2e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[0;32m      9\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstopwords\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove stopwords\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "    return text\n",
    "\n",
    "base_url = \"https://www.vanguardngr.com/category/sports\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "all_data = []\n",
    "page_num = 1\n",
    "\n",
    "while len(all_data) < 100:\n",
    "    print(f\"Scraping page {page_num}...\")\n",
    "    url = f\"{base_url}/page/{page_num}/\"\n",
    "    res = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "    # Get all article links on the page\n",
    "    articles = soup.find_all(\"h3\", class_=\"entry-title\")\n",
    "    links = [a.find(\"a\")[\"href\"] for a in articles if a.find(\"a\")]\n",
    "\n",
    "    for link in links:\n",
    "        try:\n",
    "            article_page = requests.get(link, headers=headers)\n",
    "            article_soup = BeautifulSoup(article_page.text, \"html.parser\")\n",
    "            content_div = article_soup.find(\"div\", class_=\"entry-content-inner-wrapper\")\n",
    "            if not content_div:\n",
    "                continue\n",
    "            paragraphs = content_div.find_all(\"p\")\n",
    "            for p in paragraphs:\n",
    "                raw_text = p.get_text(strip=True)\n",
    "                if len(raw_text) > 40:  # filter out short junk\n",
    "                    cleaned = clean_text(raw_text)\n",
    "                    all_data.append([raw_text, cleaned, \"Sports\", link])\n",
    "                    if len(all_data) >= 100:\n",
    "                        break\n",
    "            time.sleep(1)  # polite delay\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {link}: {e}\")\n",
    "    page_num += 1\n",
    "\n",
    "# Save to CSV\n",
    "with open(\"vanguard_sports_paragraphs.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Text\", \"Cleaned_Text\", \"Category\", \"Source\"])\n",
    "    writer.writerows(all_data)\n",
    "\n",
    "print(f\"\\n✅ Done. Saved {len(all_data)} paragraphs to vanguard_sports_paragraphs.csv.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1495ebcb-587b-4374-8724-646100b299e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HomePC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "\n",
      "✅ Done. Saved 117 paragraphs to vanguard_sports_paragraphs.csv.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Try to import nltk, install if not found\n",
    "try:\n",
    "    import nltk\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"nltk\"])\n",
    "    import nltk\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Normalize whitespace\n",
    "    text = text.lower()  # Lowercase\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)  # Remove stopwords\n",
    "    return text\n",
    "\n",
    "base_url = \"https://www.vanguardngr.com/category/sports\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "all_data = []\n",
    "page_num = 1\n",
    "\n",
    "while len(all_data) < 100:\n",
    "    print(f\"Scraping page {page_num}...\")\n",
    "    url = f\"{base_url}/page/{page_num}/\"\n",
    "    res = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "    articles = soup.find_all(\"h3\", class_=\"entry-title\")\n",
    "    links = [a.find(\"a\")[\"href\"] for a in articles if a.find(\"a\")]\n",
    "\n",
    "    for link in links:\n",
    "        try:\n",
    "            article_page = requests.get(link, headers=headers)\n",
    "            article_soup = BeautifulSoup(article_page.text, \"html.parser\")\n",
    "            content_div = article_soup.find(\"div\", class_=\"entry-content-inner-wrapper\")\n",
    "            if not content_div:\n",
    "                continue\n",
    "            paragraphs = content_div.find_all(\"p\")\n",
    "            for p in paragraphs:\n",
    "                raw_text = p.get_text(strip=True)\n",
    "                if len(raw_text) > 40:\n",
    "                    cleaned = clean_text(raw_text)\n",
    "                    all_data.append([raw_text, cleaned, \"Sports\", link])\n",
    "                    if len(all_data) >= 100:\n",
    "                        break\n",
    "            time.sleep(1)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {link}: {e}\")\n",
    "    page_num += 1\n",
    "\n",
    "# Save to CSV\n",
    "with open(\"vanguard_sports_paragraphs.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Text\", \"Cleaned_Text\", \"Category\", \"Source\"])\n",
    "    writer.writerows(all_data)\n",
    "\n",
    "print(f\"\\n✅ Done. Saved {len(all_data)} paragraphs to vanguard_sports_paragraphs.csv.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905bcab9-b6e8-46f0-9dbe-95f13ce6808f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
